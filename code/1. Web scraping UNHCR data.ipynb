{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75740233",
   "metadata": {},
   "source": [
    "# Accessing Data from UNHCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b888e",
   "metadata": {},
   "source": [
    "Input file: Syra + Refugee_1, provided from UNHCR  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c42ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import preprocessor\n",
    "from ekphrasis.classes.tokenizer import Tokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer \n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f51abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/emiliatrulsson/Desktop/Syria Preprocess/Syra + Refugees_1.csv\", delimiter=';', header=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d66365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Headline</th>\n",
       "      <th>URL</th>\n",
       "      <th>Opening Text</th>\n",
       "      <th>Hit Sentence</th>\n",
       "      <th>Source</th>\n",
       "      <th>Influencer</th>\n",
       "      <th>Country</th>\n",
       "      <th>Subregion</th>\n",
       "      <th>Language</th>\n",
       "      <th>...</th>\n",
       "      <th>Twitter Screen Name</th>\n",
       "      <th>User Profile Url</th>\n",
       "      <th>Twitter Bio</th>\n",
       "      <th>Twitter Followers</th>\n",
       "      <th>Twitter Following</th>\n",
       "      <th>Alternate Date Format</th>\n",
       "      <th>Time</th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Document Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15-Jan-2011 11:58PM</td>\n",
       "      <td>称阿拉克重水反应堆生产出氘化物 伊朗公布一项核进展</td>\n",
       "      <td>http://www.tianjinwe.com/rollnews/201101/t2011...</td>\n",
       "      <td>... 据新华社德黑兰1月15日电 （记者 何光海 杜源江）伊朗代理外长兼原子能组织主席萨利...</td>\n",
       "      <td>... 。他说：“阿拉克重水反应堆设施在氘化物方面已经实现自给自足。” 受伊朗政府邀请，阿尔...</td>\n",
       "      <td>天津网</td>\n",
       "      <td>何光海 杜源江</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>Tianjin</td>\n",
       "      <td>Chinese Simplified</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15-jan-11</td>\n",
       "      <td>11:58 em</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15-Jan-2011 11:57PM</td>\n",
       "      <td>「阪神大震災１６年 「いつか」に広域で備えよ」</td>\n",
       "      <td>http://www.iza.ne.jp/news/newsarticle/column/o...</td>\n",
       "      <td>... 【主張】 阪神淡路大震災から１７日で１６年がたつ。 ...</td>\n",
       "      <td>... 、大阪などは標識が目立たずほとんど知られていない。防災標識は規格統一すべきだ。 また...</td>\n",
       "      <td>iza：イザ！</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15-jan-11</td>\n",
       "      <td>11:57 em</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15-Jan-2011 11:56PM</td>\n",
       "      <td>Weekend Football TV Times: Jan 15-17</td>\n",
       "      <td>http://www.straitstimes.com/BreakingNews/Sport...</td>\n",
       "      <td>Jan 15 Asian Cup: Group D - Iran v North Korea...</td>\n",
       "      <td>Saudi Arabia v Japan (Live, mio TV Ch115 &amp; HD ...</td>\n",
       "      <td>The Straits Times - Singapore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15-jan-11</td>\n",
       "      <td>11:56 em</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15-Jan-2011 11:52PM</td>\n",
       "      <td>Funde von Max von Oppenheim</td>\n",
       "      <td>http://www.express.de/express/regional/koeln/d...</td>\n",
       "      <td>... [ Schließen ] zurück 1 | 5 weiter Max von ...</td>\n",
       "      <td>. Seine letzte Nahostreise unternahm der Orien...</td>\n",
       "      <td>Express.de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Nordrhein-Westfalen regional</td>\n",
       "      <td>German</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15-jan-11</td>\n",
       "      <td>11:52 em</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15-Jan-2011 11:51PM</td>\n",
       "      <td>称阿拉克重水反应堆生产出氘化物 伊朗公布一项核进展</td>\n",
       "      <td>http://news.china.com.cn/rollnews/2011-01/16/c...</td>\n",
       "      <td>... 内容摘要: （记者 何光海 杜源江）伊朗代理外长兼原子能组织主席萨利希15日说，伊朗...</td>\n",
       "      <td>... 。他说：“阿拉克重水反应堆设施在氘化物方面已经实现自给自足。” 受伊朗政府邀请，阿尔...</td>\n",
       "      <td>中国网 - 新闻中心</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Chinese Simplified</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15-jan-11</td>\n",
       "      <td>11:51 em</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date                              Headline  \\\n",
       "0  15-Jan-2011 11:58PM             称阿拉克重水反应堆生产出氘化物 伊朗公布一项核进展   \n",
       "1  15-Jan-2011 11:57PM               「阪神大震災１６年 「いつか」に広域で備えよ」   \n",
       "2  15-Jan-2011 11:56PM  Weekend Football TV Times: Jan 15-17   \n",
       "3  15-Jan-2011 11:52PM           Funde von Max von Oppenheim   \n",
       "4  15-Jan-2011 11:51PM             称阿拉克重水反应堆生产出氘化物 伊朗公布一项核进展   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  http://www.tianjinwe.com/rollnews/201101/t2011...   \n",
       "1  http://www.iza.ne.jp/news/newsarticle/column/o...   \n",
       "2  http://www.straitstimes.com/BreakingNews/Sport...   \n",
       "3  http://www.express.de/express/regional/koeln/d...   \n",
       "4  http://news.china.com.cn/rollnews/2011-01/16/c...   \n",
       "\n",
       "                                        Opening Text  \\\n",
       "0  ... 据新华社德黑兰1月15日电 （记者 何光海 杜源江）伊朗代理外长兼原子能组织主席萨利...   \n",
       "1                  ... 【主張】 阪神淡路大震災から１７日で１６年がたつ。 ...   \n",
       "2  Jan 15 Asian Cup: Group D - Iran v North Korea...   \n",
       "3  ... [ Schließen ] zurück 1 | 5 weiter Max von ...   \n",
       "4  ... 内容摘要: （记者 何光海 杜源江）伊朗代理外长兼原子能组织主席萨利希15日说，伊朗...   \n",
       "\n",
       "                                        Hit Sentence  \\\n",
       "0  ... 。他说：“阿拉克重水反应堆设施在氘化物方面已经实现自给自足。” 受伊朗政府邀请，阿尔...   \n",
       "1  ... 、大阪などは標識が目立たずほとんど知られていない。防災標識は規格統一すべきだ。 また...   \n",
       "2  Saudi Arabia v Japan (Live, mio TV Ch115 & HD ...   \n",
       "3  . Seine letzte Nahostreise unternahm der Orien...   \n",
       "4  ... 。他说：“阿拉克重水反应堆设施在氘化物方面已经实现自给自足。” 受伊朗政府邀请，阿尔...   \n",
       "\n",
       "                          Source Influencer         Country  \\\n",
       "0                            天津网    何光海 杜源江  Mainland China   \n",
       "1                        iza：イザ！        NaN           Japan   \n",
       "2  The Straits Times - Singapore        NaN       Singapore   \n",
       "3                     Express.de        NaN         Germany   \n",
       "4                     中国网 - 新闻中心        NaN  Mainland China   \n",
       "\n",
       "                      Subregion            Language  ...  Twitter Screen Name  \\\n",
       "0                       Tianjin  Chinese Simplified  ...                  NaN   \n",
       "1                           NaN            Japanese  ...                  NaN   \n",
       "2                           NaN             English  ...                  NaN   \n",
       "3  Nordrhein-Westfalen regional              German  ...                  NaN   \n",
       "4                       Beijing  Chinese Simplified  ...                  NaN   \n",
       "\n",
       "   User Profile Url  Twitter Bio  Twitter Followers  Twitter Following  \\\n",
       "0               NaN          NaN                NaN                NaN   \n",
       "1               NaN          NaN                NaN                NaN   \n",
       "2               NaN          NaN                NaN                NaN   \n",
       "3               NaN          NaN                NaN                NaN   \n",
       "4               NaN          NaN                NaN                NaN   \n",
       "\n",
       "   Alternate Date Format      Time  State City Document Tags  \n",
       "0              15-jan-11  11:58 em    NaN  NaN           NaN  \n",
       "1              15-jan-11  11:57 em    NaN  NaN           NaN  \n",
       "2              15-jan-11  11:56 em    NaN  NaN           NaN  \n",
       "3              15-jan-11  11:52 em    NaN  NaN           NaN  \n",
       "4              15-jan-11  11:51 em    NaN  NaN           NaN  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a168ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                     853083\n",
       "Headline                 853055\n",
       "URL                      853061\n",
       "Opening Text             810513\n",
       "Hit Sentence             843419\n",
       "Source                   853060\n",
       "Influencer               312794\n",
       "Country                  853060\n",
       "Subregion                289776\n",
       "Language                 853073\n",
       "Reach                    853073\n",
       "Desktop Reach            853073\n",
       "Mobile Reach             853073\n",
       "Twitter Social Echo      853073\n",
       "Facebook Social Echo     853073\n",
       "Reddit Social Echo       853073\n",
       "National Viewership      853060\n",
       "Engagement                   13\n",
       "AVE                      853073\n",
       "Sentiment                853060\n",
       "Key Phrases              714101\n",
       "Input Name               853073\n",
       "Keywords                 853060\n",
       "Twitter Authority             0\n",
       "Tweet Id                      0\n",
       "Twitter Id                    0\n",
       "Twitter Client                0\n",
       "Twitter Screen Name           0\n",
       "User Profile Url              0\n",
       "Twitter Bio                   0\n",
       "Twitter Followers             0\n",
       "Twitter Following            13\n",
       "Alternate Date Format    853073\n",
       "Time                     853060\n",
       "State                      3165\n",
       "City                       2912\n",
       "Document Tags                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4abd9afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288151\n"
     ]
    }
   ],
   "source": [
    "# Only keeping English articles by making a filter that the language column must be English\n",
    "filter = df['Language'].str.contains('English', na=False)\n",
    "count=len(df[filter])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62e8b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the dataframe to the created English filter\n",
    "df=df[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4349bbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229654"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a filter to only use the articles the has Syria as a keyword\n",
    "filter_syr=filter = df['Keywords'].str.contains('Syria', na=False)\n",
    "count_s=len(df[filter_syr])\n",
    "count_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04bf342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dataframe with the Syria keyword filter \n",
    "df_filter = df[filter_syr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31b1f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                     229654\n",
       "Headline                 229653\n",
       "URL                      229654\n",
       "Opening Text             200901\n",
       "Hit Sentence             222552\n",
       "Source                   229654\n",
       "Influencer               130864\n",
       "Country                  229654\n",
       "Subregion                 63552\n",
       "Language                 229654\n",
       "Reach                    229654\n",
       "Desktop Reach            229654\n",
       "Mobile Reach             229654\n",
       "Twitter Social Echo      229654\n",
       "Facebook Social Echo     229654\n",
       "Reddit Social Echo       229654\n",
       "National Viewership      229654\n",
       "Engagement                    0\n",
       "AVE                      229654\n",
       "Sentiment                229654\n",
       "Key Phrases              227220\n",
       "Input Name               229654\n",
       "Keywords                 229654\n",
       "Twitter Authority             0\n",
       "Tweet Id                      0\n",
       "Twitter Id                    0\n",
       "Twitter Client                0\n",
       "Twitter Screen Name           0\n",
       "User Profile Url              0\n",
       "Twitter Bio                   0\n",
       "Twitter Followers             0\n",
       "Twitter Following             0\n",
       "Alternate Date Format    229654\n",
       "Time                     229654\n",
       "State                       458\n",
       "City                        393\n",
       "Document Tags                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3b58830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file\n",
    "df_filter.to_csv('/Users/emiliatrulsson/Desktop/Syria Preprocess/syria_unhcr', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41affbae",
   "metadata": {},
   "source": [
    "# Web Scraper Process for the UNHCR Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c27e67",
   "metadata": {},
   "source": [
    "Pre-requisite files:\n",
    "\n",
    "- syria_unhcr, only English written articles with the keyword Syria from the original file provided by UNHCR\n",
    "- ukraine_unhcr, the original file provided by UNHCR\n",
    "\n",
    "For the web scraping process the same code is used for both groups, only  different files are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8100d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import timedelta\n",
    "\n",
    "def get_html_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        # check if the website allows web scraping\n",
    "        robots_url = url.split('/')[0] + '//' + url.split('/')[2] + '/robots.txt'\n",
    "        robots_response = requests.get(robots_url)\n",
    "        if 'User-agent: *\\nDisallow: /' in robots_response.text:\n",
    "            return None\n",
    "        else:\n",
    "            return response.text\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout occurred while fetching {url}, skipping...\")\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def save_html_to_file(html_content, file_name):\n",
    "    with open(file_name, 'w') as html_file:\n",
    "        html_file.write(html_content)\n",
    "\n",
    "def delete_html_file(file_name):\n",
    "    try:\n",
    "        os.remove(file_name)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def get_headline_text_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    try:\n",
    "        headline = soup.find('h1').text\n",
    "    except AttributeError:\n",
    "        headline = ''\n",
    "    try:\n",
    "        text_elements = soup.find_all('p')\n",
    "        text = '\\n'.join([element.text for element in text_elements])\n",
    "    except AttributeError:\n",
    "        text = ''\n",
    "    return headline, text\n",
    "\n",
    "def save_data_to_csv(data, file_name):\n",
    "    df = pd.DataFrame(data, columns=['headline', 'text', 'URL'])\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "def main():\n",
    "    csv_file = '/Users/emiliatrulsson/Desktop/Preprocess/syria_unhcr.csv'\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    urls = df['URL'].tolist()\n",
    "    data = []\n",
    "    start_time = time.time()\n",
    "    for i, url in enumerate(urls):\n",
    "        print(f\"Processing URL {i+1}/{len(urls)}\")\n",
    "        html_content = get_html_content(url)\n",
    "        if html_content is None:\n",
    "            print(f\"{url} does not allow web scraping\")\n",
    "            continue\n",
    "        file_name = f\"{url.split('/')[-1]}.html\"\n",
    "        if len(file_name) > 255:\n",
    "            print(f\"File name {file_name} is too long, skipping...\")\n",
    "            continue\n",
    "        save_html_to_file(html_content, file_name)\n",
    "        headline, text = get_headline_text_from_html(html_content)\n",
    "        data.append({'headline': headline, 'text': text, 'URL': url})\n",
    "        elapsed_time = time.time() - start_time\n",
    "        avg_time_per_url = elapsed_time / (i+1)\n",
    "        remaining_time = timedelta(seconds=(len(urls) - (i+1)) * avg_time_per_url)\n",
    "        print(f\"Estimated time remaining: {remaining_time}\")\n",
    "        delete_html_file(file_name)\n",
    "    save_data_to_csv(data, '/Users/emiliatrulsson/Desktop/Preprocess/syria_scraped.csv ')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
